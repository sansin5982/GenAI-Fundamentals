# Stop Words
### ๐ซ What are Stop Words?
**Stop words** are **commonly used words** in a language that **carry little meaning** on their own in the context of text analysis.

Examples in English:

`โisโ`, `โtheโ`, `โaโ`, `โofโ`, `โtoโ`, `โinโ`, `โandโ`, `โonโ`, `โitโ`, `โwithโ`

These words are usually removed during preprocessing because they **donโt contribute meaningful information** for NLP tasks like classification, clustering, or search.

### ๐ฏ Why are Stop Words Used in Text Processing?

| Purpose                      | Explanation                                          |
| ---------------------------- | ---------------------------------------------------- |
| ๐ Reduces Dimensionality    | Fewer unique words โ faster processing               |
| ๐งน Cleans the Text           | Removes irrelevant or filler content                 |
| ๐ก Focus on Keywords         | Helps algorithms concentrate on **important tokens** |
| โ Improves Model Performance | Especially in Bag-of-Words or TF-IDF models          |



```python
paragraph = """
The student was working on an interesting NLP project. It involved text preprocessing and analysis.
She was studying how to clean and prepare raw data.
Many common words in the English language were not adding much value.
So, she removed these stop words to simplify the input.
Then she used stemming to reduce words like "running", "played", and "studies".
In contrast, she also explored lemmatization to get more meaningful base words.
This helped in improving the machine learning modelโs accuracy.
The pipeline included tokenization, normalization, and vectorization.
She tested both methods and found lemmatization better for her use case.
The final model gave excellent results on real-world text data.
"""

```


```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer


```


```python
# Download required resources
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
```


```python
# Initialize
stop_words = set(stopwords.words("english"))

stop_words
```




    {'a',
     'about',
     'above',
     'after',
     'again',
     'against',
     'ain',
     'all',
     'am',
     'an',
     'and',
     'any',
     'are',
     'aren',
     "aren't",
     'as',
     'at',
     'be',
     'because',
     'been',
     'before',
     'being',
     'below',
     'between',
     'both',
     'but',
     'by',
     'can',
     'couldn',
     "couldn't",
     'd',
     'did',
     'didn',
     "didn't",
     'do',
     'does',
     'doesn',
     "doesn't",
     'doing',
     'don',
     "don't",
     'down',
     'during',
     'each',
     'few',
     'for',
     'from',
     'further',
     'had',
     'hadn',
     "hadn't",
     'has',
     'hasn',
     "hasn't",
     'have',
     'haven',
     "haven't",
     'having',
     'he',
     "he'd",
     "he'll",
     "he's",
     'her',
     'here',
     'hers',
     'herself',
     'him',
     'himself',
     'his',
     'how',
     'i',
     "i'd",
     "i'll",
     "i'm",
     "i've",
     'if',
     'in',
     'into',
     'is',
     'isn',
     "isn't",
     'it',
     "it'd",
     "it'll",
     "it's",
     'its',
     'itself',
     'just',
     'll',
     'm',
     'ma',
     'me',
     'mightn',
     "mightn't",
     'more',
     'most',
     'mustn',
     "mustn't",
     'my',
     'myself',
     'needn',
     "needn't",
     'no',
     'nor',
     'not',
     'now',
     'o',
     'of',
     'off',
     'on',
     'once',
     'only',
     'or',
     'other',
     'our',
     'ours',
     'ourselves',
     'out',
     'over',
     'own',
     're',
     's',
     'same',
     'shan',
     "shan't",
     'she',
     "she'd",
     "she'll",
     "she's",
     'should',
     "should've",
     'shouldn',
     "shouldn't",
     'so',
     'some',
     'such',
     't',
     'than',
     'that',
     "that'll",
     'the',
     'their',
     'theirs',
     'them',
     'themselves',
     'then',
     'there',
     'these',
     'they',
     "they'd",
     "they'll",
     "they're",
     "they've",
     'this',
     'those',
     'through',
     'to',
     'too',
     'under',
     'until',
     'up',
     've',
     'very',
     'was',
     'wasn',
     "wasn't",
     'we',
     "we'd",
     "we'll",
     "we're",
     "we've",
     'were',
     'weren',
     "weren't",
     'what',
     'when',
     'where',
     'which',
     'while',
     'who',
     'whom',
     'why',
     'will',
     'with',
     'won',
     "won't",
     'wouldn',
     "wouldn't",
     'y',
     'you',
     "you'd",
     "you'll",
     "you're",
     "you've",
     'your',
     'yours',
     'yourself',
     'yourselves'}




```python
stopwords.words("arabic")
```




    ['ุฅุฐ',
     'ุฅุฐุง',
     'ุฅุฐูุง',
     'ุฅุฐู',
     'ุฃู',
     'ุฃูู',
     'ุฃูุซุฑ',
     'ุฃูุง',
     'ุฅูุง',
     'ุงูุชู',
     'ุงูุฐู',
     'ุงูุฐูู',
     'ุงููุงุชู',
     'ุงููุงุฆู',
     'ุงููุชุงู',
     'ุงููุชูุง',
     'ุงููุชูู',
     'ุงููุฐุงู',
     'ุงููุฐูู',
     'ุงูููุงุชู',
     'ุฅูู',
     'ุฅููู',
     'ุฅูููู',
     'ุฅููููุง',
     'ุฅูููู',
     'ุฃู',
     'ุฃูุง',
     'ุฃูุง',
     'ุฅูุง',
     'ุฃู',
     'ุฅู',
     'ุฅูุง',
     'ุฃูุง',
     'ุฃูุช',
     'ุฃูุชู',
     'ุฃูุชูุง',
     'ุฃูุชู',
     'ุฅููุง',
     'ุฅูู',
     'ุฃูู',
     'ุฃูู',
     'ุขู',
     'ุขูุง',
     'ุฃู',
     'ุฃููุงุก',
     'ุฃููุฆู',
     'ุฃูู',
     'ุขู',
     'ุฃู',
     'ุฃููุง',
     'ุฅู',
     'ุฃูู',
     'ุฃูู',
     'ุฃูููุง',
     'ุฅูู',
     'ุจุฎ',
     'ุจุณ',
     'ุจุนุฏ',
     'ุจุนุถ',
     'ุจู',
     'ุจูู',
     'ุจูู',
     'ุจููุง',
     'ุจูู',
     'ุจู',
     'ุจูู',
     'ุจูุง',
     'ุจูุงุฐุง',
     'ุจูู',
     'ุจูุง',
     'ุจู',
     'ุจูุง',
     'ุจูู',
     'ุจููุง',
     'ุจูู',
     'ุจู',
     'ุจูู',
     'ุจูุฏ',
     'ุชูู',
     'ุชููู',
     'ุชูููุง',
     'ุชู',
     'ุชู',
     'ุชูู',
     'ุชููู',
     'ุซู',
     'ุซูุฉ',
     'ุญุงุดุง',
     'ุญุจุฐุง',
     'ุญุชู',
     'ุญูุซ',
     'ุญูุซูุง',
     'ุญูู',
     'ุฎูุง',
     'ุฏูู',
     'ุฐุง',
     'ุฐุงุช',
     'ุฐุงู',
     'ุฐุงู',
     'ุฐุงูู',
     'ุฐูู',
     'ุฐููู',
     'ุฐูููุง',
     'ุฐููู',
     'ุฐู',
     'ุฐู',
     'ุฐูุง',
     'ุฐูุงุชุง',
     'ุฐูุงุชู',
     'ุฐู',
     'ุฐูู',
     'ุฐููู',
     'ุฑูุซ',
     'ุณูู',
     'ุณูู',
     'ุดุชุงู',
     'ุนุฏุง',
     'ุนุณู',
     'ุนู',
     'ุนูู',
     'ุนููู',
     'ุนููู',
     'ุนูุง',
     'ุนู',
     'ุนูุฏ',
     'ุบูุฑ',
     'ูุฅุฐุง',
     'ูุฅู',
     'ููุง',
     'ููู',
     'ูู',
     'ููู',
     'ูููุง',
     'ููู',
     'ูููุง',
     'ูุฏ',
     'ูุฃู',
     'ูุฃููุง',
     'ูุฃู',
     'ูุฃูู',
     'ูุฐุง',
     'ูุฐูู',
     'ูู',
     'ููุง',
     'ููุงููุง',
     'ููุชุง',
     'ูููุง',
     'ูููููุง',
     'ูููููุง',
     'ูู',
     'ูู',
     'ููุง',
     'ูู',
     'ููุช',
     'ููู',
     'ููููุง',
     'ูุง',
     'ูุงุณููุง',
     'ูุฏู',
     'ูุณุช',
     'ูุณุชู',
     'ูุณุชูุง',
     'ูุณุชู',
     'ูุณู',
     'ูุณูุง',
     'ูุนู',
     'ูู',
     'ููู',
     'ูููุง',
     'ููู',
     'ููููุง',
     'ููู',
     'ููููุง',
     'ูู',
     'ููุง',
     'ูู',
     'ููุง',
     'ูู',
     'ููุง',
     'ููู',
     'ูููุง',
     'ููู',
     'ูู',
     'ูููุง',
     'ูููุง',
     'ูู',
     'ูุฆู',
     'ููุช',
     'ููุณ',
     'ููุณุง',
     'ููุณุช',
     'ููุณุชุง',
     'ููุณูุง',
     'ูุง',
     'ูุงุฐุง',
     'ูุชู',
     'ูุฐ',
     'ูุน',
     'ููุง',
     'ููู',
     'ูู',
     'ููู',
     'ูููุง',
     'ููุฐ',
     'ูู',
     'ูููุง',
     'ูุญู',
     'ูุญู',
     'ูุนู',
     'ูุง',
     'ูุงุชุงู',
     'ูุงุชู',
     'ูุงุชู',
     'ูุงุชูู',
     'ูุงู',
     'ูุงููุง',
     'ูุฐุง',
     'ูุฐุงู',
     'ูุฐู',
     'ูุฐู',
     'ูุฐูู',
     'ููุฐุง',
     'ูู',
     'ููุง',
     'ูู',
     'ููุง',
     'ูู',
     'ููุง',
     'ููุงู',
     'ููุงูู',
     'ูู',
     'ูุคูุงุก',
     'ูู',
     'ููุง',
     'ููุช',
     'ูููุงุช',
     'ูุงูุฐู',
     'ูุงูุฐูู',
     'ูุฅุฐ',
     'ูุฅุฐุง',
     'ูุฅู',
     'ููุง',
     'ูููู',
     'ููู',
     'ููุง',
     'ููู',
     'ููู',
     'ูุง',
     'ุฃุจู',
     'ุฃุฎู',
     'ุญูู',
     'ูู',
     'ุฃูุชู',
     'ููุงูุฑ',
     'ูุจุฑุงูุฑ',
     'ูุงุฑุณ',
     'ุฃุจุฑูู',
     'ูุงูู',
     'ููููู',
     'ููููู',
     'ุฃุบุณุทุณ',
     'ุณุจุชูุจุฑ',
     'ุฃูุชูุจุฑ',
     'ููููุจุฑ',
     'ุฏูุณูุจุฑ',
     'ุฌุงููู',
     'ูููุฑู',
     'ูุงุฑุณ',
     'ุฃูุฑูู',
     'ูุงู',
     'ุฌูุงู',
     'ุฌููููุฉ',
     'ุฃูุช',
     'ูุงููู',
     'ุดุจุงุท',
     'ุขุฐุงุฑ',
     'ููุณุงู',
     'ุฃูุงุฑ',
     'ุญุฒูุฑุงู',
     'ุชููุฒ',
     'ุขุจ',
     'ุฃูููู',
     'ุชุดุฑูู',
     'ุฏููุงุฑ',
     'ุฏููุงุฑ',
     'ุฑูุงู',
     'ุฏุฑูู',
     'ููุฑุฉ',
     'ุฌููู',
     'ูุฑุด',
     'ูููู',
     'ููุณ',
     'ูููุฉ',
     'ุณูุชูู',
     'ููุฑู',
     'ูู',
     'ููุงู',
     'ุดููู',
     'ูุงุญุฏ',
     'ุงุซูุงู',
     'ุซูุงุซุฉ',
     'ุฃุฑุจุนุฉ',
     'ุฎูุณุฉ',
     'ุณุชุฉ',
     'ุณุจุนุฉ',
     'ุซูุงููุฉ',
     'ุชุณุนุฉ',
     'ุนุดุฑุฉ',
     'ุฃุญุฏ',
     'ุงุซูุง',
     'ุงุซูู',
     'ุฅุญุฏู',
     'ุซูุงุซ',
     'ุฃุฑุจุน',
     'ุฎูุณ',
     'ุณุช',
     'ุณุจุน',
     'ุซูุงูู',
     'ุชุณุน',
     'ุนุดุฑ',
     'ุซูุงู',
     'ุณุจุช',
     'ุฃุญุฏ',
     'ุงุซููู',
     'ุซูุงุซุงุก',
     'ุฃุฑุจุนุงุก',
     'ุฎููุณ',
     'ุฌูุนุฉ',
     'ุฃูู',
     'ุซุงู',
     'ุซุงูู',
     'ุซุงูุซ',
     'ุฑุงุจุน',
     'ุฎุงูุณ',
     'ุณุงุฏุณ',
     'ุณุงุจุน',
     'ุซุงูู',
     'ุชุงุณุน',
     'ุนุงุดุฑ',
     'ุญุงุฏู',
     'ุฃ',
     'ุจ',
     'ุช',
     'ุซ',
     'ุฌ',
     'ุญ',
     'ุฎ',
     'ุฏ',
     'ุฐ',
     'ุฑ',
     'ุฒ',
     'ุณ',
     'ุด',
     'ุต',
     'ุถ',
     'ุท',
     'ุธ',
     'ุน',
     'ุบ',
     'ู',
     'ู',
     'ู',
     'ู',
     'ู',
     'ู',
     'ู',
     'ู',
     'ู',
     'ุก',
     'ู',
     'ุข',
     'ุค',
     'ุฆ',
     'ุฃ',
     'ุฉ',
     'ุฃูู',
     'ุจุงุก',
     'ุชุงุก',
     'ุซุงุก',
     'ุฌูู',
     'ุญุงุก',
     'ุฎุงุก',
     'ุฏุงู',
     'ุฐุงู',
     'ุฑุงุก',
     'ุฒุงู',
     'ุณูู',
     'ุดูู',
     'ุตุงุฏ',
     'ุถุงุฏ',
     'ุทุงุก',
     'ุธุงุก',
     'ุนูู',
     'ุบูู',
     'ูุงุก',
     'ูุงู',
     'ูุงู',
     'ูุงู',
     'ููู',
     'ููู',
     'ูุงุก',
     'ูุงู',
     'ูุงุก',
     'ููุฒุฉ',
     'ู',
     'ูุง',
     'ู',
     'ูู',
     'ู',
     'ุฅูุงู',
     'ุฅูุงูุง',
     'ุฅูุงููุง',
     'ุฅูุงูู',
     'ุฅูุงูู',
     'ุฅูุงู',
     'ุฅูุงููุง',
     'ุฅูุงูู',
     'ุฅูุงู',
     'ุฅูุงูู',
     'ุฅูุงู',
     'ุฅูุงูุง',
     'ุฃููุงูู',
     'ุชุงูู',
     'ุชุงููู',
     'ุชูู',
     'ุชูู',
     'ุชููููู',
     'ุซูู',
     'ุซููุฉ',
     'ุฐุงูู',
     'ุฐูู',
     'ุฐูู',
     'ุฐููููู',
     'ููุคูุงุก',
     'ููุงุชุงูู',
     'ููุงุชูู',
     'ููุงุชูู',
     'ููุงุชููููู',
     'ููุฐุง',
     'ููุฐุงูู',
     'ููุฐูู',
     'ููุฐูู',
     'ููุฐููููู',
     'ุงูุฃูู',
     'ุงูุฃูุงุก',
     'ุฃู',
     'ุฃููู',
     'ุฃูู',
     'ูุฃููุงู',
     'ุฃููู',
     'ุฃูู',
     'ูุฃููุงู',
     'ุฐูุช',
     'ูุฃูู',
     'ูุฃููู',
     'ุจุถุน',
     'ููุงู',
     'ูุง',
     'ุขูููู',
     'ุขูู',
     'ุขูู',
     'ุขูุงู',
     'ุฃูููู',
     'ุฃูููู',
     'ุฃููู',
     'ุฃูุงูู',
     'ุฃูุงููู',
     'ุฃูููู',
     'ุฅูููููู',
     'ุฅูููููู',
     'ุฅูููู',
     'ุฅููููู',
     'ุฅููู',
     'ุจุฎู',
     'ุจุณู',
     'ุจูุณู',
     'ุจุทุขู',
     'ุจููููู',
     'ุญุงู',
     'ุญูุฐุงุฑู',
     'ุญููู',
     'ุญููู',
     'ุฏููู',
     'ุฑููุฏู',
     'ุณุฑุนุงู',
     'ุดุชุงูู',
     'ุดูุชููุงูู',
     'ุตูู',
     'ุตูู',
     'ุทุงู',
     'ุทูู',
     'ุนูุฏูุณู',
     'ููุฎ',
     'ููุงููู',
     'ููุงููู',
     'ููุงููู',
     'ููุงููู',
     'ููุงูููุง',
     'ููุงูููู',
     'ููุฎู',
     'ูุงูู',
     'ููุฌู',
     'ููู',
     'ูููุง',
     'ูููููุงุช',
     'ูุง',
     'ูุงูุงู',
     'ูุฑุงุกูู',
     'ููุดูููุงูู',
     'ูููู',
     'ููุนูุงู',
     'ุชูุนูุงู',
     'ููุนููู',
     'ุชูุนููู',
     'ุชูุนููู',
     'ุงุชุฎุฐ',
     'ุฃููู',
     'ุชุฎุฐ',
     'ุชุฑู',
     'ุชุนูููู',
     'ุฌุนู',
     'ุญุฌุง',
     'ุญุจูุจ',
     'ุฎุงู',
     'ุญุณุจ',
     'ุฎุงู',
     'ุฏุฑู',
     'ุฑุฃู',
     'ุฒุนู',
     'ุตุจุฑ',
     'ุธููู',
     'ุนุฏูู',
     'ุนูู',
     'ุบุงุฏุฑ',
     'ุฐูุจ',
     'ูุฌุฏ',
     'ูุฑุฏ',
     'ููุจ',
     'ุฃุณูู',
     'ุฃุทุนู',
     'ุฃุนุทู',
     'ุฑุฒู',
     'ุฒูุฏ',
     'ุณูู',
     'ูุณุง',
     'ุฃุฎุจุฑ',
     'ุฃุฑู',
     'ุฃุนูู',
     'ุฃูุจุฃ',
     'ุญุฏูุซ',
     'ุฎุจููุฑ',
     'ูุจููุง',
     'ุฃูุนู ุจู',
     'ูุง ุฃูุนูู',
     'ุจุฆุณ',
     'ุณุงุก',
     'ุทุงููุง',
     'ูููุง',
     'ูุงุช',
     'ููููู',
     'ุกู',
     'ุฃุฌู',
     'ุฅุฐุงู',
     'ุฃููุง',
     'ุฅููุง',
     'ุฅููู',
     'ุฃููู',
     'ุฃู',
     'ุฅู',
     'ุฃูุง',
     'ุจ',
     'ุซููู',
     'ุฌูู',
     'ุฌูุฑ',
     'ุฑูุจูู',
     'ุณ',
     'ุนููู',
     'ู',
     'ูุฃูู',
     'ููููุง',
     'ูู',
     'ู',
     'ูุงุช',
     'ูุนููู',
     'ููููู',
     'ููููู',
     'ู',
     'ููู',
     'ูููุง',
     'ูุง',
     'ุฃู',
     'ุฅููุง',
     'ุช',
     'ู',
     'ูููุง',
     'ู',
     'ู',
     'ู',
     'ุง',
     'ู',
     'ุชุฌุงู',
     'ุชููุงุก',
     'ุฌููุน',
     'ุญุณุจ',
     'ุณุจุญุงู',
     'ุดุจู',
     'ูุนูุฑ',
     'ูุซู',
     'ูุนุงุฐ',
     'ุฃุจู',
     'ุฃุฎู',
     'ุญูู',
     'ูู',
     'ูุฆุฉ',
     'ูุฆุชุงู',
     'ุซูุงุซูุฆุฉ',
     'ุฃุฑุจุนูุฆุฉ',
     'ุฎูุณูุฆุฉ',
     'ุณุชูุฆุฉ',
     'ุณุจุนูุฆุฉ',
     'ุซูููุฆุฉ',
     'ุชุณุนูุฆุฉ',
     'ูุงุฆุฉ',
     'ุซูุงุซูุงุฆุฉ',
     'ุฃุฑุจุนูุงุฆุฉ',
     'ุฎูุณูุงุฆุฉ',
     'ุณุชูุงุฆุฉ',
     'ุณุจุนูุงุฆุฉ',
     'ุซูุงููุฆุฉ',
     'ุชุณุนูุงุฆุฉ',
     'ุนุดุฑูู',
     'ุซูุงุซูู',
     'ุงุฑุจุนูู',
     'ุฎูุณูู',
     'ุณุชูู',
     'ุณุจุนูู',
     'ุซูุงููู',
     'ุชุณุนูู',
     'ุนุดุฑูู',
     'ุซูุงุซูู',
     'ุงุฑุจุนูู',
     'ุฎูุณูู',
     'ุณุชูู',
     'ุณุจุนูู',
     'ุซูุงููู',
     'ุชุณุนูู',
     'ุจุถุน',
     'ููู',
     'ุฃุฌูุน',
     'ุฌููุน',
     'ุนุงูุฉ',
     'ุนูู',
     'ููุณ',
     'ูุง ุณููุง',
     'ุฃุตูุง',
     'ุฃููุง',
     'ุฃูุถุง',
     'ุจุคุณุง',
     'ุจุนุฏุง',
     'ุจุบุชุฉ',
     'ุชุนุณุง',
     'ุญูุง',
     'ุญูุฏุง',
     'ุฎูุงูุง',
     'ุฎุงุตุฉ',
     'ุฏูุงููู',
     'ุณุญูุง',
     'ุณุฑุง',
     'ุณูุนุง',
     'ุตุจุฑุง',
     'ุตุฏูุง',
     'ุตุฑุงุญุฉ',
     'ุทุฑุง',
     'ุนุฌุจุง',
     'ุนูุงูุง',
     'ุบุงูุจุง',
     'ูุฑุงุฏู',
     'ูุถูุง',
     'ูุงุทุจุฉ',
     'ูุซูุฑุง',
     'ูุจูู',
     'ูุนุงุฐ',
     'ุฃุจุฏุง',
     'ุฅุฒุงุก',
     'ุฃุตูุง',
     'ุงูุขู',
     'ุฃูุฏ',
     'ุฃูุณ',
     'ุขููุง',
     'ุขูุงุก',
     'ุฃููู',
     'ุฃูู',
     'ุฃููุงู',
     'ุชุงุฑุฉ',
     'ุซูู',
     'ุซููุฉ',
     'ุญูุง',
     'ุตุจุงุญ',
     'ูุณุงุก',
     'ุถุญูุฉ',
     'ุนูุถ',
     'ุบุฏุง',
     'ุบุฏุงุฉ',
     'ูุทู',
     'ููููุง',
     'ูุฏู',
     'ูููุง',
     'ูุฑูุฉ',
     'ูุจู',
     'ุฎูู',
     'ุฃูุงู',
     'ููู',
     'ุชุญุช',
     'ูููู',
     'ุดูุงู',
     'ุงุฑุชุฏู',
     'ุงุณุชุญุงู',
     'ุฃุตุจุญ',
     'ุฃุถุญู',
     'ุขุถ',
     'ุฃูุณู',
     'ุงูููุจ',
     'ุจุงุช',
     'ุชุจุฏูู',
     'ุชุญููู',
     'ุญุงุฑ',
     'ุฑุฌุน',
     'ุฑุงุญ',
     'ุตุงุฑ',
     'ุธูู',
     'ุนุงุฏ',
     'ุบุฏุง',
     'ูุงู',
     'ูุง ุงููู',
     'ูุง ุจุฑุญ',
     'ูุงุฏุงู',
     'ูุงุฒุงู',
     'ูุงูุชุฆ',
     'ุงุจุชุฏุฃ',
     'ุฃุฎุฐ',
     'ุงุฎูููู',
     'ุฃูุจู',
     'ุงูุจุฑู',
     'ุฃูุดุฃ',
     'ุฃูุดู',
     'ุฌุนู',
     'ุญุฑู',
     'ุดุฑุน',
     'ุทูู',
     'ุนูู',
     'ูุงู',
     'ูุฑุจ',
     'ูุงุฏ',
     'ูุจู']




```python
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

```


```python
# Tokenize paragraph into words
words = word_tokenize(paragraph)
```


```python
# Output
print("Original Words:\n", words)
```

    Original Words:
     ['The', 'student', 'was', 'working', 'on', 'an', 'interesting', 'NLP', 'project', '.', 'It', 'involved', 'text', 'preprocessing', 'and', 'analysis', '.', 'She', 'was', 'studying', 'how', 'to', 'clean', 'and', 'prepare', 'raw', 'data', '.', 'Many', 'common', 'words', 'in', 'the', 'English', 'language', 'were', 'not', 'adding', 'much', 'value', '.', 'So', ',', 'she', 'removed', 'these', 'stop', 'words', 'to', 'simplify', 'the', 'input', '.', 'Then', 'she', 'used', 'stemming', 'to', 'reduce', 'words', 'like', '``', 'running', "''", ',', '``', 'played', "''", ',', 'and', '``', 'studies', "''", '.', 'In', 'contrast', ',', 'she', 'also', 'explored', 'lemmatization', 'to', 'get', 'more', 'meaningful', 'base', 'words', '.', 'This', 'helped', 'in', 'improving', 'the', 'machine', 'learning', 'model', 'โ', 's', 'accuracy', '.', 'The', 'pipeline', 'included', 'tokenization', ',', 'normalization', ',', 'and', 'vectorization', '.', 'She', 'tested', 'both', 'methods', 'and', 'found', 'lemmatization', 'better', 'for', 'her', 'use', 'case', '.', 'The', 'final', 'model', 'gave', 'excellent', 'results', 'on', 'real-world', 'text', 'data', '.']
    


```python
# Remove stop words
filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]
print("\nFiltered Words (Stop Words Removed):\n", filtered_words)
```

    
    Filtered Words (Stop Words Removed):
     ['student', 'working', 'interesting', 'NLP', 'project', 'involved', 'text', 'preprocessing', 'analysis', 'studying', 'clean', 'prepare', 'raw', 'data', 'Many', 'common', 'words', 'English', 'language', 'adding', 'much', 'value', 'removed', 'stop', 'words', 'simplify', 'input', 'used', 'stemming', 'reduce', 'words', 'like', 'running', 'played', 'studies', 'contrast', 'also', 'explored', 'lemmatization', 'get', 'meaningful', 'base', 'words', 'helped', 'improving', 'machine', 'learning', 'model', 'accuracy', 'pipeline', 'included', 'tokenization', 'normalization', 'vectorization', 'tested', 'methods', 'found', 'lemmatization', 'better', 'use', 'case', 'final', 'model', 'gave', 'excellent', 'results', 'text', 'data']
    


```python
stemmed = [stemmer.stem(word) for word in filtered_words]
print("\nStemmed Words:\n", stemmed)
```

    
    Stemmed Words:
     ['student', 'work', 'interest', 'nlp', 'project', 'involv', 'text', 'preprocess', 'analysi', 'studi', 'clean', 'prepar', 'raw', 'data', 'mani', 'common', 'word', 'english', 'languag', 'ad', 'much', 'valu', 'remov', 'stop', 'word', 'simplifi', 'input', 'use', 'stem', 'reduc', 'word', 'like', 'run', 'play', 'studi', 'contrast', 'also', 'explor', 'lemmat', 'get', 'meaning', 'base', 'word', 'help', 'improv', 'machin', 'learn', 'model', 'accuraci', 'pipelin', 'includ', 'token', 'normal', 'vector', 'test', 'method', 'found', 'lemmat', 'better', 'use', 'case', 'final', 'model', 'gave', 'excel', 'result', 'text', 'data']
    


```python
lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]
print("\nLemmatized Words:\n", lemmatized)
```

    
    Lemmatized Words:
     ['student', 'working', 'interesting', 'NLP', 'project', 'involved', 'text', 'preprocessing', 'analysis', 'studying', 'clean', 'prepare', 'raw', 'data', 'Many', 'common', 'word', 'English', 'language', 'adding', 'much', 'value', 'removed', 'stop', 'word', 'simplify', 'input', 'used', 'stemming', 'reduce', 'word', 'like', 'running', 'played', 'study', 'contrast', 'also', 'explored', 'lemmatization', 'get', 'meaningful', 'base', 'word', 'helped', 'improving', 'machine', 'learning', 'model', 'accuracy', 'pipeline', 'included', 'tokenization', 'normalization', 'vectorization', 'tested', 'method', 'found', 'lemmatization', 'better', 'use', 'case', 'final', 'model', 'gave', 'excellent', 'result', 'text', 'data']
    
