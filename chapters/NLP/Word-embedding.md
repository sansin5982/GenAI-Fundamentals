## Word Embedding

In NLP, word embedding is a term used for the **representation of words** for text analysis, typically in the form of a **real-valued vector** that encodes the meaning of the word such that the words that are closer in the vector space are expected to be **similar in meaning** (wikipedia).

* Happy, Excited, Angry
* Here happy and excited are quite similar in meaning. Angry is opposite
* If we creare a 2D plot using PCA or something else. Happy and excited will fall close to each other, suggesting words are similar. Angry will be away from both words suggesting different meaning.

### Word Embedding
* All of these **convert words into vector** and **sentnece into word**
* Specifically of two types:

    * 1. Based on count or frequency
        * **OHE**
        * **BOW**
        * **TF_IDF**
    * 2. DL trained model
        * **Word2Vec**
            * **CBOW** (continuous BAG of Words)
            * **skipgram**
            


```python

```
