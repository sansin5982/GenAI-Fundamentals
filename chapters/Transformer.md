# Transformer
The **Transformer** is a deep learning architecture introduced in the 2017 paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al. It was developed for **natural language processing (NLP)** tasks but is now widely used in vision, audio, and multimodal AI.

> üß† Core idea: Instead of processing sequences step-by-step (like RNNs), the Transformer processes entire sequences in parallel using self-attention.

## Architecture Components
At its core, the **Transformer architecture** from ‚ÄúAttention Is All You Need‚Äù (Vaswani et al., 2017) consists of:

* **N stacked encoders**: For understanding input sequences.
* **N stacked decoders**: For generating output sequences.

These components work **together** in sequence-to-sequence (seq2seq) tasks like translation. But in many real-world models, we often use **only one of the two**, depending on the task.

### üîπ Transformer Encoder
#### üì¶ Function: Understand and represent the input
* **Input**: Tokenized sequence (e.g., "Delhi is the capital of India.")

* **Output**: Rich vector representations of the entire sentence

#### üîß Components in each encoder block:
1. **Multi-head self-attention**: Each word "attends" to all others
2. **Feed-forward network (FFN)**: Applies a non-linear transformation
3. **Residual + LayerNorm**: Stabilizes training

#### üîÑ Parallelizable
Since the encoder looks at all tokens at once, we can process entire sequences simultaneously (unlike RNNs/LSTMs).

### üîπ Transformer Decoder
#### üì¶ Function: Generate output one token at a time (auto-regressively)
* **Input**: Previously generated output tokens (e.g., during training: ‚ÄúBonjour‚Äù ‚Üí ‚ÄúBonjour le‚Äù)

* **Output**: Next word prediction

#### üîß Components in each decoder block:
1. **Masked multi-head self-attention**: Only looks at previous tokens (prevents "cheating")
2. **Encoder-decoder attention**: Attends to the encoder‚Äôs output
3. **Feed-forward network**
4. **Residual + LayerNorm**

### üîÄ Key Difference in Behavior
| Feature                         | Encoder (BERT)                      | Decoder (GPT)                      |
| ------------------------------- | ----------------------------------- | ---------------------------------- |
| Attention                       | Unmasked (sees all tokens)          | Masked (sees only previous tokens) |
| Output                          | Embeddings (contextualized)         | Generated tokens                   |
| Use-case                        | Text understanding (classification) | Text generation                    |
| Encoder-Decoder Cross-Attention | ‚ùå Not used                          | ‚úÖ Used in full seq2seq tasks       |




### üß† Why BERT Uses Only the Encoder
#### ‚úÖ Task Type: Understanding-based
* BERT is trained using **masked language modeling**.
    * Example: ‚ÄúDelhi is the [MASK] of India.‚Äù
    * The model **predicts the missing word** based on **full context** (both left and right).

It needs **bidirectional context**, which is only possible with **unmasked attention** (from the encoder).

> üîé Think of BERT as a reader: it tries to understand full text before making a decision.

#### üéØ Common BERT Tasks:
* Sentiment classification
* Question answering (e.g., SQuAD)
* Named entity recognition


### üß† Why GPT Uses Only the Decoder
#### ‚úÖ Task Type: Generation-based
* GPT is trained using **causal language modeling** (predict next word).
    * Example: ‚ÄúDelhi is the capital of‚Äù
    * The model predicts: ‚ÄúIndia‚Äù
* Uses **masked attention** so it **can‚Äôt peek into future words**.
* The decoder structure suits **auto-regressive generation**: one word at a time.

> üó£Ô∏è Think of GPT as a writer: it generates text, one token after another, based on what it has already written.

#### üéØ Common GPT Tasks:
* Text completion
* Dialogue systems
* Story generation
* Code generation (e.g., Codex)

### üîÅ Encoder-Decoder in Full Transformers (e.g., T5, BART)
These models use **both** encoder and decoder:

* **Encoder**: Understands the input (e.g., a sentence in English)
* **Decoder**: **Generates** the output (e.g., sentence in French)

> Perfect for tasks like **translation, summarization, paraphrasing**

#### Summary Table
| Model   | Uses Encoder | Uses Decoder | Suitable For                   |
| ------- | ------------ | ------------ | ------------------------------ |
| BERT    | ‚úÖ            | ‚ùå            | Understanding (classification) |
| GPT     | ‚ùå            | ‚úÖ            | Generation (completion)        |
| BART/T5 | ‚úÖ            | ‚úÖ            | Sequence-to-sequence tasks     |


```python

```
