# ğŸ”· What Is Attention in General?
In any attention mechanism, we compute:

$$
\large Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt d_k})V
$$

But over what subset of the tokens we compute that attention defines whether itâ€™s global or local.

## ğŸ”¹ Global Attention
#### ğŸ§  Layman Explanation:
> Each token can **see and attend to every other token** in the sequence â€” full context.

ğŸ§‘â€ğŸ« Analogy:
A student listens to **everyoneâ€™s answer** in the class before writing his own.

#### ğŸ“Œ Key Characteristics:
* **Full context** â€” attends to all tokens
* Memory intensive (scales as $O(n^2)$)
* Used in original Transformer
* Can be slow for very long sequences

#### âœ… Example:
In the sentence:

> â€œIndia is the seventh-largest country by area.â€

â†’ The word â€œcountryâ€ attends to â€œIndiaâ€, â€œseventh-largestâ€, and even â€œby areaâ€.


```python
# Q, K, V are [batch, seq_len, d_model]
scores = torch.matmul(Q, K.transpose(-2, -1))  # shape: [B, L, L]
weights = F.softmax(scores, dim=-1)
output = torch.matmul(weights, V)
```

## ğŸ”¹ Local Attention
#### ğŸ§  Layman Explanation:
> Each token can only see and attend to **a fixed-size window of nearby tokens** â€” limited context.

#### ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Analogy:
A student listens only to his **neighbors** to avoid being distracted by the whole class.

#### ğŸ“Œ Key Characteristics:
* Attention is restricted to a window (e.g., Â±2 tokens)
* Scales linearly with sequence length: $O(n \cdot w)$ where $w$ is window size
* Efficient for long sequences (e.g., documents, genomes)
* Used in models like Longformer, BigBird

âœ… Example:
In a 5-token sequence:

["The", "sky", "is", "blue", "today"]

If local attention window = 3:
* "blue" only attends to ["is", "blue", "today"]
* Canâ€™t attend to "The" or "sky"

#### ğŸ§® Mathematical View
| Type   | Computation Scope                                           |
| ------ | ----------------------------------------------------------- |
| Global | All tokens: $Q \cdot K^T$ of size $n \times n$              |
| Local  | Only windowed tokens: e.g., for token i, use $i-w$ to $i+w$ |




```python
# Create a local attention mask (1s for nearby, 0s for distant)
# Example: mask[i, j] = 1 if abs(i - j) <= window_size
masked_scores = scores.masked_fill(mask == 0, float('-inf'))
weights = F.softmax(masked_scores, dim=-1)
output = torch.matmul(weights, V)
```

### ğŸ“Š Tabular Comparison
| Feature            | Global Attention                      | Local Attention                    |
| ------------------ | ------------------------------------- | ---------------------------------- |
| Visibility         | Attends to all tokens                 | Attends to nearby tokens only      |
| Computational Cost | $O(n^2)$                              | $O(n \cdot w)$, w = window size    |
| Memory Usage       | High                                  | Lower                              |
| Use Cases          | Short to medium-length text, GPT/BERT | Long documents, DNA, audio, vision |
| Example Models     | BERT, GPT, T5                         | Longformer, BigBird, Reformer      |

### ğŸ§  Summary in 1 Line
> Global Attention = â€œI see everyone.â€
> Local Attention = â€œI see only my neighbors.â€


```python

```
