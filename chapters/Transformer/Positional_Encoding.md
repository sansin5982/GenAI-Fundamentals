## üîπ Step 2: Positional Encoding (Why do we need this?)
#### ü§î Problem:
Transformers **do not know word order** by default.
They treat the input as a **set**, not a **sequence**.

That means:

* "India is great" and "Great is India" look the same ‚Äî not ideal.

#### üß≠ Solution:
We add **positional encoding** to tell the model:

> ‚ÄúThis word is at position 0‚Äù, ‚ÄúThis one is at position 1‚Äù, etc.

The positional encoding for each token position $pos$ and embedding dimension $i$ is defined as:

For even dimensions (2i):

$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

For odd dimensions (2i + 1):

$$
PE(pos, 2i + 1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

Where:

- $pos$ is the position of the word in the sequence (starting from 0),
- $i$ is the dimension index,
- $d_{\text{model}}$ is the total number of embedding dimensions.

#### Example 

Let‚Äôs say $d_{\text{model}} = 4$  you have 3 positions:

#### üßÆ Manual Positional Encoding (d=4)
Let‚Äôs compute PE for each position (pos = 0, 1, 2):

* **Position 0 (India)**

$$
\large  PE(0)=[sin(0),cos(0),sin(0),cos(0)]=[0,1,0,1]
$$

* **Position 1 (is)**

    * $$PE(1,0)=sin(1/10000^{0/4})=sin(1)‚âà0.841$$
    * $$PE(1,1)=cos(1/10000^{0/4})=cos(1)‚âà0.540$$
    * $$PE(1,2)=sin(1/10000^{0.5})‚âàsin(\frac{1}{100})‚âà0.0.00999$$
    * $$PE(1,3)=cos(1/10000^{0.5})‚âàcos(\frac{1}{100})‚âà0.0.9999$$

So:

$$
\large PE(1)=[0.841,0.540,0.00999,0.9999]
$$

*  **Position 2 (great)**
  * $$sin(2) ‚âà 0.909, cos(2) ‚âà -0.416$$
  * $$sin(\frac{2}{100}) ‚âà 0.02, cos(\frac{2}{100}) ‚âà -0.9988$$ 

$$
\large PE(2)=[0.909,‚àí0.416,0.02,0.9998]
$$

| Position | PE vector                   |
| -------- | --------------------------- |
| 0        | \[0.0, 1.0, 0.0, 1.0]       |
| 1        | \[0.84, 0.54, 0.00999, 0.9999]   |
| 2        | \[0.909, -0.4016, 0.02, 0.9908] |

#### üßÆ Final Input to the Model
For each word:

$$
\large Input_{\text{final}} =  Embedding(word) + PositionalEncoding(position)
$$

This sum is **element-wise** (vector + vector):

Example for word "India" at position 0:

**Embedding** = [0.45, -0.32, 0.12, 0.99]

**PositionalEncoding** = [0.0, 1.0, 0.0, 1.0]

**FinalInput** = [0.45, 0.68, 0.12, 1.99]

#### üß† Why Add Instead of Concatenate?
* Addition keeps the vector size fixed.
* It‚Äôs computationally more efficient.
* Positional patterns blend naturally with word meanings.

#### Summary (Layman View)
| Concept             | Meaning                                            |
| ------------------- | -------------------------------------------------- |
| Embedding           | Converts words to vectors the computer understands |
| Positional Encoding | Adds "position sense" to the vectors               |
| Final Input Vector  | A mix of word meaning and word position            |


### EXAMPLE
#### ‚ú≥Ô∏è Sentence: "India is great"
The goal is to convert each word into a **vector that combines its meaning and position**.

#### üî∂ Step 1: Token Embedding
Each word is passed through a token embedding layer, which maps it to a fixed-size vector.

| Token | Word Embedding (Vector)|
| ----- | --------------------- |
| India | \[0.1, 0.3, 0.4, 0.5] |
| is    | \[0.2, 0.1, 0.6, 0.3] |
| great | \[0.4, 0.3, 0.9, 0.1] |

These numbers represent the **semantic meaning** of each word, learned during training.

#### üü£ Step 2: Positional Encoding
Since Transformers don‚Äôt understand order by default, we add **positional information**:

| Position | Positional Encoding Vector (Sample) |
| -------- | ----------------------------------- |
| 0        | `[0.00, 1.00, 0.00, 1.00]`          |
| 1        | `[0.84, 0.54, 0.009, 1.00]`         |
| 2        | `[1.82, -0.42, 0.018, 0.99]`        |

These vectors are deterministic and based on sinusoids. They help the model know **which word came first, second, etc**.

#### ‚úÖ Step 3: Final Input Vector = Embedding + PositionalEncoding
Each vector is added element-wise (one number at a time):

#### Example: "India" (Position 0)

**[0.45,‚àí0.32,0.12,0.99]+[0.00,1.00,0.00,1.00] = [0.45,0.68,0.12,1.99]**

Likewise, for `"is"` and `"great"`:

* "is" at position 1 ‚Üí `[0.22 + 0.84, 0.11 + 0.54, ...]`

* "great" at position 2 ‚Üí `[0.91 + 1.82, -0.44 + (-0.42), ...]`

#### Final Embeddings:
| Token | Word Embedding        | Positional Encoding              | Final Vector                     |
| ----- | --------------------- | -------------------------------- | -------------------------------- |
| India | \[0.1, 0.3, 0.4, 0.5] | \[0, 1, 0, 1]                    | \[0.1, 1.3, 0.4, 1.5]            |
| is    | \[0.2, 0.1, 0.6, 0.3] | \[0.841, 0.540, 0.00999, 0.9999] | \[1.041, 0.640, 0.60999, 1.2999] |
| great | \[0.4, 0.3, 0.9, 0.1] | \[0.909, -0.416, 0.02, 0.9998]   | \[1.309, -0.116, 0.92, 1.0998]   |


#### üß† Why This Matters
This step ensures the Transformer:

* Understands what the word means (via embedding)
* Knows where the word appears (via position)

Together, these inputs are sent into the first **encoder or decoder** block in the Transformer model.


```python

```
